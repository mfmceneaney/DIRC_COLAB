{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIRC_CNN_Architecture_Evaluation_V7_MASTER.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfmceneaney/DIRC_COLAB/blob/master/DIRC_CNN_Architecture_Evaluation_V7_MASTER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-uxFFigUIQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using GPU speeds up CNN training\n",
        "# Go to Edit > Notebook Settings > Hardware Accelerator and select GPU (TPU is very slow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HpK-suowGUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\" > Installing uproot...\")\n",
        "!pip install uproot\n",
        "\n",
        "print(' > Installing h5py...')\n",
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvCGwbOcT6oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import uproot\n",
        "\n",
        "from os import chdir, getcwd, mkdir, listdir, rmdir\n",
        "from shutil import rmtree\n",
        "\n",
        "# Standard scientific Python imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Graphics Imports\n",
        "import matplotlib as mpl\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x \n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, metrics, regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import DenseNet121, ResNet50V2, VGG16, VGG19, MobileNet, MobileNetV2, NASNetLarge, NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam, Nadam, RMSprop, SGD\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "# !rm -rf ./logs/ \n",
        "\n",
        "# Imports for custom callbacks\n",
        "from tensorflow.keras.callbacks import Callback, ProgbarLogger\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "#### Check out this link for tf.keras vs. tf.python.keras : https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0sofRSRTWfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "chdir('/content/gdrive/My Drive/DIRC_CNN/') # This directory created 1/22/20 18:33:00\n",
        "listdir()\n",
        "chdir('test') # Store files in the test directory temporarily in case something goes wrong\n",
        "listdir()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcZW_VUxhUUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get data from files\n",
        "print(\"Opening files\")\n",
        "rmtree('Data_Split_Test')\n",
        "mkdir('Data_Split_Test')\n",
        "chdir('Data_Split_Test')\n",
        "\n",
        "# 3GeV\n",
        "piplus3t4 = uproot.open(\"/content/gdrive/My Drive/piplus_p3_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus3t4 = uproot.open(\"/content/gdrive/My Drive/kplus_p3_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p3_theta4')\n",
        "# chdir('p3_theta4')\n",
        "\n",
        "# 4GeV\n",
        "piplus4t4 = uproot.open(\"/content/gdrive/My Drive/piplus_p4_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus4t4 = uproot.open(\"/content/gdrive/My Drive/kplus_p4_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p4_theta4')\n",
        "# chdir('p4_theta4')\n",
        "\n",
        "# 5GeV\n",
        "piplus5t4 = uproot.open(\"/content/gdrive/My Drive/piplus_p5_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus5t4 = uproot.open(\"/content/gdrive/My Drive/kplus_p5_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p5_theta4')\n",
        "# chdir('p5_theta4')\n",
        "\n",
        "# 6GeV\n",
        "piplus6t4 = uproot.open(\"/content/gdrive/My Drive/piplus_p6_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus6t4 = uproot.open(\"/content/gdrive/My Drive/kplus_p6_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p6_theta4')\n",
        "# chdir('p6_theta4')\n",
        "\n",
        "# 7GeV\n",
        "piplus7t4 = uproot.open(\"/content/gdrive/My Drive/tree_piplus_p7_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus7t4 = uproot.open(\"/content/gdrive/My Drive/tree_kplus_p7_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p7_theta4')\n",
        "# chdir('p7_theta4')\n",
        "\n",
        "# 8GeV\n",
        "piplus8t4 = uproot.open(\"/content/gdrive/My Drive/tree_piplus_p8_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus8t4 = uproot.open(\"/content/gdrive/My Drive/tree_kplus_p8_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p8_theta4')\n",
        "# chdir('p8_theta4')\n",
        "\n",
        "# 9GeV\n",
        "piplus9t4 = uproot.open(\"/content/gdrive/My Drive/tree_piplus_p9_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus9t4 = uproot.open(\"/content/gdrive/My Drive/tree_kplus_p9_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p9_theta4')\n",
        "# chdir('p9_theta4')\n",
        "\n",
        "# 10GeV\n",
        "piplus10t4 = uproot.open(\"/content/gdrive/My Drive/tree_piplus_p10_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus10t4 = uproot.open(\"/content/gdrive/My Drive/tree_kplus_p10_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p10_theta4')\n",
        "# chdir('p10_theta4')\n",
        "\n",
        "# 11GeV\n",
        "piplus11t4 = uproot.open(\"/content/gdrive/My Drive/tree_piplus_p11_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus11t4 = uproot.open(\"/content/gdrive/My Drive/tree_kplus_p11_theta4_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p11_theta4')\n",
        "# chdir('p11_theta4')\n",
        "\n",
        "################################## PHI -45 ##################################\n",
        "# # 3GeV\n",
        "# mom = '3GeV'\n",
        "piplus3t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p3_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus3t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p3_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# # # rmtree('p3_theta4_phim45')\n",
        "# # mkdir('p3_theta4_phim45')\n",
        "# chdir('p3_theta4_phim45')\n",
        "# listdir()\n",
        "\n",
        "# 4GeV\n",
        "# mom = '4GeV'\n",
        "piplus4t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p4_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus4t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p4_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p4_theta4_phim45')\n",
        "# chdir('p4_theta4_phim45')\n",
        "# listdir()\n",
        "\n",
        "# 5GeV\n",
        "# mom = '5GeV'\n",
        "piplus5t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p5_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus5t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p5_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p5_theta4_phim45')\n",
        "# chdir('p5_theta4_phim45')\n",
        "\n",
        "# # 6GeV\n",
        "# mom = '6GeV'\n",
        "piplus6t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p6_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus6t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p6_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p6_theta4_phim45')\n",
        "# chdir('p6_theta4_phim45')\n",
        "\n",
        "# # 7GeV\n",
        "# mom = '7GeV'\n",
        "piplus7t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p7_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus7t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p7_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p7_theta4_phim45')\n",
        "# chdir('p7_theta4_phim45')\n",
        "\n",
        "# # 8GeV\n",
        "# mom = '8GeV'\n",
        "piplus8t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p8_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus8t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p8_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p8_theta4_phim45')\n",
        "# chdir('p8_theta4_phim45')\n",
        "\n",
        "# # 9GeV\n",
        "# mom = '9GeV'\n",
        "piplus9t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p9_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus9t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p9_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p9_theta4_phim45')\n",
        "# chdir('p9_theta4_phim45')\n",
        "# listdir()\n",
        "\n",
        "# # 10GeV\n",
        "# mom = '10GeV'\n",
        "piplus10t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p10_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus10t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p10_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p10_theta4_phim45')\n",
        "# chdir('p10_theta4_phim45')\n",
        "\n",
        "# # 11GeV\n",
        "# mom = '11GeV'\n",
        "piplus11t4phim45 = uproot.open(\"/content/gdrive/My Drive/piplus_p11_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "kplus11t4phim45 = uproot.open(\"/content/gdrive/My Drive/kplus_p11_theta4_phim45_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p11_theta4_phim45')\n",
        "# chdir('p11_theta4_phim45')\n",
        "\n",
        "\n",
        "################################## THETA 8 ##################################\n",
        "# # 3GeV\n",
        "# mom = '3GeV'\n",
        "piplus3t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p3_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus3t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p3_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p3_theta8')\n",
        "# chdir('p3_theta8')\n",
        "\n",
        "# # 4GeV\n",
        "# mom = '4GeV'\n",
        "piplus4t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p4_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus4t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p4_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p4_theta8')\n",
        "# chdir('p4_theta8')\n",
        "# listdir()\n",
        "\n",
        "# # 5GeV\n",
        "# mom = '5GeV'\n",
        "piplus5t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p5_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus5t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p5_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p5_theta8')\n",
        "# chdir('p5_theta8')\n",
        "\n",
        "# # 6GeV\n",
        "# mom = '6GeV'\n",
        "piplus6t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p6_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus6t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p6_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p6_theta8')\n",
        "# chdir('p6_theta8')\n",
        "\n",
        "# # 7GeV\n",
        "# mom = '7GeV'\n",
        "piplus7t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p7_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus7t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p7_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p7_theta8')\n",
        "# chdir('p7_theta8')\n",
        "\n",
        "# # 8GeV\n",
        "# mom = '8GeV'\n",
        "piplus8t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p8_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus8t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p8_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p8_theta8')\n",
        "# chdir('p8_theta8')\n",
        "\n",
        "# # 9GeV\n",
        "# mom = '9GeV'\n",
        "piplus9t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p9_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus9t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p9_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# # mkdir('p9_theta8')\n",
        "# chdir('p9_theta8')\n",
        "# listdir()\n",
        "\n",
        "# # 10GeV\n",
        "# mom = '10GeV'\n",
        "piplus10t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p10_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus10t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p10_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('cross_check')\n",
        "# chdir('cross_check')\n",
        "\n",
        "# # 11GeV\n",
        "# mom = '11GeV'\n",
        "piplus11t8 = uproot.open(\"/content/gdrive/My Drive/piplus_p11_theta8_flat.root\")[\"dircml_flat\"]\n",
        "kplus11t8 = uproot.open(\"/content/gdrive/My Drive/kplus_p11_theta8_flat.root\")[\"dircml_flat\"]\n",
        "# mkdir('p11_theta8')\n",
        "# chdir('p11_theta8')\n",
        "\n",
        "\n",
        "data = [[[kplus3t4,piplus3t4],\n",
        "        [kplus4t4,piplus4t4],\n",
        "        [kplus5t4,piplus5t4],\n",
        "        [kplus6t4,piplus6t4],\n",
        "        [kplus7t4,piplus7t4],\n",
        "        [kplus8t4,piplus8t4],\n",
        "        [kplus9t4,piplus9t4],\n",
        "        [kplus10t4,piplus10t4],\n",
        "        [kplus11t4,piplus11t4]],\n",
        "        [[kplus3t4phim45,piplus3t4phim45],\n",
        "        [kplus4t4phim45,piplus4t4phim45],\n",
        "        [kplus5t4phim45,piplus5t4phim45],\n",
        "        [kplus6t4phim45,piplus6t4phim45],\n",
        "        [kplus7t4phim45,piplus7t4phim45],\n",
        "        [kplus8t4phim45,piplus8t4phim45],\n",
        "        [kplus9t4phim45,piplus9t4phim45],\n",
        "        [kplus10t4phim45,piplus10t4phim45],\n",
        "        [kplus11t4phim45,piplus11t4phim45]],\n",
        "        [[kplus3t8,piplus3t8],\n",
        "        [kplus4t8,piplus4t8],\n",
        "        [kplus5t8,piplus5t8],\n",
        "        [kplus6t8,piplus6t8],\n",
        "        [kplus7t8,piplus7t8],\n",
        "        [kplus8t8,piplus8t8],\n",
        "        [kplus9t8,piplus9t8],\n",
        "        [kplus10t8,piplus10t8],\n",
        "        [kplus11t8,piplus11t8]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99DmdI8VVHXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer data to appropriately shaped np arrays\n",
        "images_kplus = []\n",
        "images_piplus = []\n",
        "images_flat_kplus = []\n",
        "images_flat_piplus = []\n",
        "\n",
        "times_kplus = []\n",
        "times_kplus_flat = []\n",
        "times_piplus = []\n",
        "times_piplus_flat = []\n",
        "\n",
        "# initialize images\n",
        "image_kplus = np.zeros(shape=(48,144,2))\n",
        "image_piplus = np.zeros(shape=(48,144,2))\n",
        "\n",
        "def get_data(kplus,piplus,time_cut,eventMax,num_channels=2,images_kplus=images_kplus,images_piplus=images_piplus,times_kplus=times_kplus,times_piplus=times_piplus):\n",
        "\n",
        "  eventMax = eventMax  # originially 10k\n",
        "  eventCounter = 0\n",
        "\n",
        "  print(\"Filling image arrays\")\n",
        "  # loop over kplus events\n",
        "  for (PixelTimes,PixelRows,PixelCols) in zip(kplus.array(\"PixelTime\"),kplus.array(\"PixelRow\"),kplus.array(\"PixelCol\")):\n",
        "      #print(\"event\")\n",
        "      image_single = np.zeros(shape=(48,144,num_channels))\n",
        "      times_single = []\n",
        "      \n",
        "      # loop over pixels within event\n",
        "      for (PixelTime,PixelRow,PixelCol) in zip(PixelTimes,PixelRows,PixelCols):\n",
        "          #print(\"PixelTime,Row,Col = %f,%d,%d\" % (PixelTime,PixelRow,PixelCol))\n",
        "          times_single.append(PixelTime)\n",
        "          if PixelTime < time_cut:\n",
        "            image_kplus[PixelCol,PixelRow,0] += 1\n",
        "          elif PixelTime >= time_cut:\n",
        "            image_kplus[PixelCol,PixelRow,1] += 1\n",
        "          image_single[PixelCol,PixelRow] = PixelTime\n",
        "          #h2_kplus.Fill(PixelRow,PixelCol)\n",
        "      \n",
        "      # after each event\n",
        "      images_kplus.append(image_single)\n",
        "      # images_flat_kplus.append(np.reshape(image_single, 6912))\n",
        "\n",
        "      times_kplus += times_single\n",
        "      \n",
        "      eventCounter = eventCounter+1\n",
        "      if eventCounter > eventMax:\n",
        "          break\n",
        "\n",
        "  eventCounter = 0\n",
        "          \n",
        "  # loop over piplus events\n",
        "  for (PixelTimes,PixelRows,PixelCols) in zip(piplus.array(\"PixelTime\"),piplus.array(\"PixelRow\"),piplus.array(\"PixelCol\")):\n",
        "      #print(\"event\")\n",
        "      image_single2 = np.zeros(shape=(48,144,num_channels))\n",
        "      times_single = []\n",
        "\n",
        "      # loop over pixels within event\n",
        "      for (PixelTime,PixelRow,PixelCol) in zip(PixelTimes,PixelRows,PixelCols):\n",
        "          #print(\"PixelTime,Row,Col = %f,%d,%d\" % (PixelTime,PixelRow,PixelCol))\n",
        "          times_single.append(PixelTime)\n",
        "          if PixelTime < time_cut:\n",
        "            image_piplus[PixelCol,PixelRow,0] += 1\n",
        "          elif PixelTime >= time_cut:\n",
        "            image_piplus[PixelCol,PixelRow,1] += 1\n",
        "          image_single2[PixelCol,PixelRow] = PixelTime\n",
        "          #h2_piplus.Fill(PixelRow,PixelCol)\n",
        "      \n",
        "      # after each event\n",
        "      images_piplus.append(image_single)\n",
        "      # images_flat_piplus.append(np.reshape(image_single, 6912))\n",
        "      \n",
        "      times_piplus += times_single\n",
        "\n",
        "      eventCounter = eventCounter+1\n",
        "      if eventCounter > eventMax:\n",
        "          break\n",
        "\n",
        "\n",
        "for entry in data[0]:\n",
        "  get_data(entry[0],entry[1],50,1000)\n",
        "\n",
        "for entry in data[1]:\n",
        "  get_data(entry[0],entry[1],65,1000)\n",
        "\n",
        "for entry in data[2]:\n",
        "  get_data(entry[0],entry[1],50,1000)\n",
        "\n",
        "print(images_kplus[0].shape)\n",
        "print(len(times_kplus))\n",
        "print(np.amax(times_kplus))\n",
        "print(np.amin(times_kplus))\n",
        "print(len(times_piplus))\n",
        "print(np.amax(times_piplus))\n",
        "print(np.amin(times_piplus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxwtrYWaxub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw cumulative channel 1 images\n",
        "f = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "ax1 = plt.gca()\n",
        "plt.title(\"All Angles Channel 1 (t<50ns) Hit Pattern K+ (top) and Pi+ (bottom)\")\n",
        "im1 = plt.imshow(image_kplus[:,:,0], cmap='viridis') #, vmin=0, vmax=500)\n",
        "divider1 = make_axes_locatable(ax1)\n",
        "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb1 = plt.colorbar(im1, cax=cax1)\n",
        "cb1.set_label(\"Hits\")\n",
        "cb1.minorticks_on()\n",
        "plt.subplot(2, 1, 2)\n",
        "ax2 = plt.gca()\n",
        "plt.title(\"\")\n",
        "im2 = plt.imshow(image_piplus[:,:,0], cmap='viridis') #, vmin=0, vmax=500\n",
        "divider2 = make_axes_locatable(ax2)\n",
        "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb2 = plt.colorbar(im2, cax=cax2)\n",
        "cb2.set_label(\"Hits\")\n",
        "cb2.minorticks_on()\n",
        "plt.show()\n",
        "f.savefig('kp_pip_ch1_cumulative.png')\n",
        "\n",
        "# draw cumulative channel 2 images\n",
        "f = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "ax1 = plt.gca()\n",
        "plt.title(\"All Angles Channel 2 (t>=50ns) Hit Pattern K+ (top) and Pi+ (bottom)\")\n",
        "im1 = plt.imshow(image_kplus[:,:,1], cmap='viridis') #, vmin=0, vmax=500\n",
        "divider1 = make_axes_locatable(ax1)\n",
        "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb1 = plt.colorbar(im1, cax=cax1)\n",
        "cb1.set_label(\"Hits\")\n",
        "cb1.minorticks_on()\n",
        "plt.subplot(2, 1, 2)\n",
        "ax2 = plt.gca()\n",
        "plt.title(\"\")\n",
        "im2 = plt.imshow(image_piplus[:,:,1], cmap='viridis') #, vmin=0, vmax=500\n",
        "divider2 = make_axes_locatable(ax2)\n",
        "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb2 = plt.colorbar(im2, cax=cax2)\n",
        "cb2.set_label(\"Hits\")\n",
        "cb2.minorticks_on()\n",
        "plt.show()\n",
        "f.savefig('kp_pip_ch2_cumulative.png')\n",
        "\n",
        "# draw single channel 1 images\n",
        "f = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "ax1 = plt.gca()\n",
        "plt.title(\"All Angles Channel 1 (t<50ns) Hit Pattern K+ (top) and Pi+ (bottom)\")\n",
        "im1 = plt.imshow(images_kplus[1][:,:,0], cmap='viridis')\n",
        "divider1 = make_axes_locatable(ax1)\n",
        "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb1 = plt.colorbar(im1, cax=cax1)\n",
        "cb1.set_label(\"Hits\")\n",
        "cb1.minorticks_on()\n",
        "plt.subplot(2, 1, 2)\n",
        "ax2 = plt.gca()\n",
        "plt.title(\"\")\n",
        "im2 = plt.imshow(images_piplus[1][:,:,0], cmap='viridis', vmin=0, vmax=500) #, vmin=0, vmax=500\n",
        "divider2 = make_axes_locatable(ax2)\n",
        "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb2 = plt.colorbar(im2, cax=cax2)\n",
        "cb2.set_label(\"Hits\")\n",
        "cb2.minorticks_on()\n",
        "plt.show()\n",
        "f.savefig('kp_pip_ch1_single.png')\n",
        "\n",
        "# draw single channel 2 images\n",
        "f = plt.figure()\n",
        "plt.subplot(2, 1, 1)\n",
        "ax1 = plt.gca()\n",
        "plt.title(\"All Angles Channel 2 (t>=50ns) Hit Pattern K+ (top) and Pi+ (bottom)\")\n",
        "im1 = plt.imshow(images_kplus[1][:,:,1], cmap='viridis')\n",
        "divider1 = make_axes_locatable(ax1)\n",
        "cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb1 = plt.colorbar(im1, cax=cax1)\n",
        "cb1.set_label(\"Hits\")\n",
        "cb1.minorticks_on()\n",
        "plt.subplot(2, 1, 2)\n",
        "ax2 = plt.gca()\n",
        "plt.title(\"\")\n",
        "im2 = plt.imshow(images_piplus[1][:,:,1], cmap='viridis', vmin=0, vmax=500) #, vmin=0, vmax=500\n",
        "divider2 = make_axes_locatable(ax2)\n",
        "cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "cb2 = plt.colorbar(im2, cax=cax2)\n",
        "cb2.set_label(\"Hits\")\n",
        "cb2.minorticks_on()\n",
        "plt.show()\n",
        "f.savefig('kp_pip_ch2_single.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US7KP1Olo2ag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preshuffle data\n",
        "preShuffle = True\n",
        "if preShuffle:\n",
        "\n",
        "  # Preshuffle DIRC Data\n",
        "  np.random.shuffle(images_kplus)\n",
        "  np.random.shuffle(images_piplus)\n",
        "  print(len(images_kplus))\n",
        "  print(len(images_piplus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ4-MSLja1iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split samples into training and testing\n",
        "print(\"Splitting sample into training and testing subsets\")\n",
        "split = np.array((50,25,25)) # Percentages into which to split data (% train, % val, % test)\n",
        "n_kplus = len(images_kplus)\n",
        "n_piplus = len(images_piplus)\n",
        "n_total = n_kplus + n_piplus\n",
        "print(n_kplus / split)\n",
        "\n",
        "print(n_kplus // split)\n",
        "print(n_piplus // split)\n",
        "images_train_kplus = images_kplus[:int(n_kplus * split[0] / 100)]\n",
        "images_train_piplus = images_piplus[:int(n_piplus * split[0] / 100)]\n",
        "n_train_kplus = len(images_train_kplus)\n",
        "n_train_piplus = len(images_train_piplus)\n",
        "n_train_total = n_train_kplus + n_train_piplus\n",
        "\n",
        "images_val_kplus = images_kplus[int(n_kplus * split[0] / 100):int(n_piplus * (split[0] + split[1]) / 100)]\n",
        "images_val_piplus = images_piplus[int(n_piplus * split[0] / 100):int(n_piplus * (split[0] + split[1]) / 100)]\n",
        "n_val_kplus = len(images_val_kplus)\n",
        "n_val_piplus = len(images_val_piplus)\n",
        "n_val_total = n_val_kplus + n_val_piplus\n",
        "\n",
        "images_test_kplus = images_kplus[int(n_piplus * (split[0] + split[1]) / 100):]\n",
        "images_test_piplus = images_piplus[int(n_piplus * (split[0] + split[1]) / 100):]\n",
        "n_test_kplus = len(images_test_kplus)\n",
        "n_test_piplus = len(images_test_piplus)\n",
        "n_test_total = n_test_kplus + n_test_piplus\n",
        "\n",
        "data_train = np.concatenate((images_train_kplus, images_train_piplus))\n",
        "data_val = np.concatenate((images_val_kplus, images_val_piplus))\n",
        "data_test = np.concatenate((images_test_kplus, images_test_piplus))\n",
        "data = np.concatenate((images_kplus, images_piplus))\n",
        "\n",
        "target_train = np.concatenate((np.ones(n_train_kplus), np.zeros(n_train_piplus)))\n",
        "target_val = np.concatenate((np.ones(n_val_kplus), np.zeros(n_val_piplus)))\n",
        "target_test = np.concatenate((np.ones(n_test_kplus), np.zeros(n_test_piplus)))\n",
        "target = np.concatenate((np.ones(n_kplus), np.zeros(n_piplus)))\n",
        "\n",
        "print('data_train shape =  '+str(data_train.shape))\n",
        "print('data shape = '+str(data.shape))\n",
        "print('target shape = '+str(target.shape))\n",
        "print('unique elements of target = '+str(np.unique(target)))\n",
        "\n",
        "print(\"Training size = %d\" % len(data_train))\n",
        "print(\"Validation size = %d\" % len(data_val))\n",
        "print(\"Testing size = %d\" % len(data_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BETYfnkURpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize pixel absolute values to be between 0 and 1\n",
        "data_train = data_train / data.max()\n",
        "data_val = data_val / data.max()\n",
        "data_test = data_test / data.max()\n",
        "\n",
        "print(\"Data shape: \"+str(data_train.shape))\n",
        "print(\"Target shape: \"+str(target_train.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2IwthkZ0ONz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# Model Definitions #\n",
        "# Matthew McEneaney #\n",
        "# 1/12/20           #\n",
        "#####################\n",
        "num_channels = 2\n",
        "num_classes = 2\n",
        "############################################### TensorFlow Simple CNN ###########################################\n",
        "\n",
        "# CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "model = models.Sequential()\n",
        "# Padding='same' significantly improves results\n",
        "# model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "model.add(layers.Conv2D(16, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, num_channels)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "# model.add(layers.Dropout(0.1))\n",
        "# model.add(layers.Dense(16, activation='relu'))\n",
        "# model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "tf_cnn_basic = model\n",
        "print('1')\n",
        "############################################### TensorFlow Simple CNN ###########################################\n",
        "\n",
        "# CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "model = models.Sequential()\n",
        "# Padding='same' significantly improves results\n",
        "# model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, num_channels)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "# model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "# model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "tf_cnn_simple = model\n",
        "print('2')\n",
        "############################################### TensorFlow Simple CNN with Dropout ###########################################\n",
        "\n",
        "# CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "model = models.Sequential()\n",
        "# Padding='same' significantly improves results\n",
        "# model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, num_channels)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "tf_cnn_simple_wDO = model\n",
        "print('3')\n",
        "############################################### TensorFlow Simple CNN with Dropout and L2 Regularization ###########################################\n",
        "\n",
        "# CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "model = models.Sequential()\n",
        "# Padding='same' significantly improves results\n",
        "# model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, num_channels),kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "tf_cnn_simple_wDOandL2 = model\n",
        "print('4')\n",
        "############################################### TensorFlow Example Architecture ###########################################\n",
        "# check out https://www.tensorflow.org/tutorials/images/cnn\n",
        "# CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "model = models.Sequential()\n",
        "# Padding='same' significantly improves results\n",
        "# model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, num_channels)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "# model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "tf_cnn_example = model\n",
        "\n",
        "################################### Simple CNN for CIFAR-10 Dataset, use Adam Optimizer #######################################\n",
        "# Check out https://keras.io/examples/cifar10_cnn/ for examples of this architecture and others\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_example = model\n",
        "\n",
        "# # Example used this optimizer\n",
        "# # initiate RMSprop optimizer\n",
        "# opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "####################################### Try Bottleneck Layers ############################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (1, 1))) # Bottleneck layers\n",
        "model.add(Conv2D(64, (3, 3))) # ...\n",
        "model.add(Conv2D(64, (1, 1))) # ...\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cnn_bottelneck = model\n",
        "\n",
        "#################################### Try Factorization Layers #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (1, 3))) # Factorization layers\n",
        "model.add(Conv2D(64, (3, 1))) # ...\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (1, 3))) # Factorization layers\n",
        "model.add(Conv2D(64, (3, 1))) # ...\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_factorization = model\n",
        "\n",
        "#################################### Try L2 Regularization #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_regularized = model\n",
        "\n",
        "#################################### Try L2 Regularization greater value #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_reg_g = model\n",
        "\n",
        "#################################### Try L2 Regularization with more parallel #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(128, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(128, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_reg_par = model\n",
        "\n",
        "#################################### Try L2 Regularization with Larger CNN #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_reg_conv3 = model\n",
        "\n",
        "#################################### Try L2 Regularization with Larger CNN #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_reg_conv4 = model\n",
        "\n",
        "#################################### Try ELU Activation with L2 Regularization #########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "          input_shape=(48, 144, num_channels)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Conv2D(64, (3, 3),\n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, \n",
        "          kernel_regularizer=regularizers.l2(0.00001)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "tf_cifar_regwELU = model\n",
        "\n",
        "#####################################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YRM8l1oIuLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from tensorflow.python.keras.utils.generic_utils import Progbar\n",
        "#@keras_export('keras.callbacks.ProgbarLogger')\n",
        "from tensorflow.python.keras.callbacks import ProgbarLogger\n",
        "class MetricsLogger(ProgbarLogger):\n",
        "  \"\"\"Callback that prints metrics to stdout. \n",
        "  \n",
        "  Modified by Matthew McEneaney, 2/1/2020 10:00:00.\n",
        "  Arguments:\n",
        "      count_mode: One of \"steps\" or \"samples\".\n",
        "          Whether the progress bar should\n",
        "          count samples seen or steps (batches) seen.\n",
        "      stateful_metrics: Iterable of string names of metrics that\n",
        "          should *not* be averaged over an epoch.\n",
        "          Metrics in this list will be logged as-is.\n",
        "          All others will be averaged over time (e.g. loss, etc).\n",
        "  Raises:\n",
        "      ValueError: In case of invalid `count_mode`.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, logmaster, count_mode='samples', stateful_metrics=None):\n",
        "    super(MetricsLogger, self).__init__() # ORIGINAL NOT WORKING AS OF 4/1, ATTRIBUTE ERROR WITH PARAMS, SUSPECTED VERSION DISCREPANCY\n",
        "    # super().__init__()\n",
        "    if count_mode == 'samples':\n",
        "      self.use_steps = False\n",
        "    elif count_mode == 'steps':\n",
        "      self.use_steps = True\n",
        "    else:\n",
        "      raise ValueError('Unknown `count_mode`: ' + str(count_mode))\n",
        "    self.stateful_metrics = set(stateful_metrics or [])\n",
        "    self.log_values = None\n",
        "    self.log_master = logmaster\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self.verbose = self.params['verbose']\n",
        "    self.epochs = self.params['epochs']\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "    self.seen = 0\n",
        "    if self.use_steps:\n",
        "      self.target = self.params['steps']\n",
        "    # else:\n",
        "      # self.target = self.params['samples']\n",
        "\n",
        "    # if self.verbose:\n",
        "    #   if self.epochs > 1:\n",
        "    #     print('Epoch %d/%d' % (epoch + 1, self.epochs))\n",
        "    # self.progbar = Progbar(\n",
        "    #     target=self.target,\n",
        "    #     verbose=self.verbose,\n",
        "    #     stateful_metrics=self.stateful_metrics,\n",
        "    #     unit_name='step' if self.use_steps else 'sample')\n",
        "\n",
        "  def on_batch_begin(self, batch, logs=None):\n",
        "    self.log_values = []\n",
        "\n",
        "  def on_batch_end(self, batch, logs=None):\n",
        "    logs = logs or {}\n",
        "    batch_size = logs.get('size', 0)\n",
        "    # In case of distribution strategy we can potentially run multiple steps\n",
        "    # at the same time, we should account for that in the `seen` calculation.\n",
        "    num_steps = logs.get('num_steps', 1)\n",
        "    if self.use_steps:\n",
        "      self.seen += num_steps\n",
        "    else:\n",
        "      self.seen += batch_size * num_steps\n",
        "\n",
        "    for k in self.params['metrics']:\n",
        "      if k in logs:\n",
        "        self.log_values.append((k, logs[k]))\n",
        "        self.log_master.append((k,logs[k]))\n",
        "\n",
        "    # Skip progbar update for the last batch;\n",
        "    # will be handled by on_epoch_end.\n",
        "    # if self.verbose and (self.target is None or self.seen < self.target):\n",
        "    #   self.progbar.update(self.seen, self.log_values)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    logs = logs or {}\n",
        "    for k in self.params['metrics']:\n",
        "      if k in logs:\n",
        "        self.log_values.append((k, logs[k]))\n",
        "    # if self.verbose:\n",
        "    #   self.progbar.update(self.seen, self.log_values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW3YbgZIkokD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################\n",
        "# Evaluation function definitions #\n",
        "# Matthew McEneaney               #\n",
        "# 1/12/20                         #\n",
        "####################################\n",
        "\n",
        "def write_summary(model,name,verbosity):\n",
        "  '''\n",
        "  Write model architecture summary into a .txt file.\n",
        "  '''\n",
        "\n",
        "  def myFunc(text):\n",
        "    outF.write(text+'\\n')\n",
        "  \n",
        "  # Write summary to file\n",
        "  outF = open(name+\"_summary.txt\", \"w\")\n",
        "  model.summary(print_fn=myFunc)\n",
        "  outF.close()\n",
        "  if verbosity == 1:\n",
        "    model.summary()\n",
        "\n",
        "def plot_metrics(name,batch_size,results,history,n,verbosity):\n",
        "  '''\n",
        "  Plot training metrics (accuracy, validation accuracy, \n",
        "  loss, validation, loss) as a function of training \n",
        "  epoch and save in .png and .root file formats.\n",
        "  '''\n",
        "  chdir('by_epoch')\n",
        "\n",
        "  # Plot as .png\n",
        "  f = plt.figure(figsize=(16,10))\n",
        "  val = plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'],'--', color=val[0].get_color())\n",
        "  val = plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'],'--', color=val[0].get_color())\n",
        "  plt.title('Model: '+name+' '+str(n)+' metrics, batch_size = '+str(batch_size)+\", test_acc = \"+str(results[1]))\n",
        "  plt.ylabel('Arbitrary Units')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'], loc='best')\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"epochs_metrics\"+str(n)+\".png\")\n",
        "\n",
        "  chdir('..')\n",
        "\n",
        "  # Convert accuracy and loss plots into ROOT histograms\n",
        "  if verbosity == 1:\n",
        "    print(\"Converting training epochs plots to ROOT...\")\n",
        "\n",
        "  epochs = np.array([i for i in range(len(history.history['accuracy']))])\n",
        "\n",
        "  chdir('root_files')\n",
        "\n",
        "  file = uproot.recreate('epochs_accuracy_'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"acc\"] = np.histogram2d(epochs,history.history['accuracy'])\n",
        "\n",
        "  file = uproot.recreate('epochs_val_accuracy'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"val_acc\"] = np.histogram2d(epochs,history.history['val_accuracy'])\n",
        "\n",
        "  file = uproot.recreate('epochs_loss'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"loss\"] = np.histogram2d(epochs,history.history['loss'])\n",
        "\n",
        "  file = uproot.recreate('epochs_val_loss'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"val_loss\"] = np.histogram2d(epochs,history.history['val_loss'],epochs)\n",
        "  \n",
        "  chdir('..')\n",
        "\n",
        "def plot_stability(metrics,name,batch_size,verbosity):\n",
        "  '''\n",
        "  Plot the test metrics (accuracy, loss, chi2) recorded for \n",
        "  several model iterations in both .png and .root file formats.\n",
        "  '''\n",
        "\n",
        "  if verbosity == 1:\n",
        "    print(\"Plotting purity and \\u03C7\\u00b2\")\n",
        "\n",
        "  # Plot accuracy\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[1],'b.', scaley = False)\n",
        "  plt.title(\"Model: \"+name+\", test acc, batch_size = \"+str(batch_size)+\", ave = \"+str(round(np.average(metrics[1]),4)))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_acc.png\")\n",
        "\n",
        "  # Plot loss\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[2],'b.', scaley = False)\n",
        "  plt.title(\"Model: \"+name+\", test loss, batch_size = \"+str(batch_size)+\", ave = \"+str(round(np.average(metrics[2]),4)))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_loss.png\")\n",
        "\n",
        "  # Plot kaon chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][0],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Kaon \\u03C7\\u00b2, batch_size = \"+str(batch_size)+\", ave = \"+str(round(np.average(metrics[3][0]),4)))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_kaon.png\")\n",
        "\n",
        "  # Plot pion chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][1],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Pion \\u03C7\\u00b2, batch_size = \"+str(batch_size)+\", ave = \"+str(round(np.average(metrics[3][1]),4)))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_pion.png\")\n",
        "\n",
        "  # Plot total chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][2],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Total \\u03C7\\u00b2, batch_size = \"+str(batch_size)+\", ave = \"+str(round(np.average(metrics[3][2]),3)))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_tot.png\")\n",
        "    \n",
        "  # Convert accuracy and loss plots into ROOT histograms\n",
        "  if verbosity == 1:\n",
        "    print(\"Converting stability plots to ROOT...\")\n",
        "\n",
        "  chdir('root_files')\n",
        "\n",
        "  file = uproot.recreate(\"stability_acc.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"acc\"] = np.histogram2d(metrics[0],metrics[1])\n",
        "\n",
        "  file = uproot.recreate(\"stability_loss.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"loss\"] = np.histogram2d(metrics[0],metrics[2])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_kaon.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2k\"] = np.histogram2d(metrics[0],metrics[3][0])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_pion.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2p\"] = np.histogram2d(metrics[0],metrics[3][1])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_tot.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2t\"] = np.histogram2d(metrics[0],metrics[3][2])\n",
        "\n",
        "  chdir('..')\n",
        "\n",
        "\n",
        "def evaluate_model(model,metrics,results,n,verbosity):\n",
        "\n",
        "  '''\n",
        "  Record the decisions of the CNN (kaon or pion) and \n",
        "  test metrics (accuracy and loss) for a single training\n",
        "  iteration. Also calculate and record the chi2 and error\n",
        "  for kaon, pion decisions and all decisions respectively.\n",
        "  '''\n",
        "\n",
        "  # Keep track of results for stability plots\n",
        "  metrics[0].append(n)\n",
        "  metrics[1].append(results[1])\n",
        "  metrics[2].append(results[0])\n",
        "\n",
        "  # Evaluate performance by comparing test and training classifier (model) response\n",
        "  decisions = []\n",
        "  for (X,y) in ((data_train, target_train), (data_test, target_test)):\n",
        "      d1 = model.predict_proba(X[y>0.5])[:,1].ravel()\n",
        "      d2 = model.predict_proba(X[y<0.5])[:,1].ravel()\n",
        "      decisions += [d1, d2]\n",
        "      \n",
        "  print(\"Filling histograms...\")\n",
        "\n",
        "  bins = 100\n",
        "  low = min(np.min(d) for d in decisions)\n",
        "  high = max(np.max(d) for d in decisions)\n",
        "  low_high = (low,high)\n",
        "  f = plt.figure()\n",
        "  plt.clf()\n",
        "  # Plot training decisions\n",
        "  plt.hist(decisions[0], color='r', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=True, label='Kaon (train)')\n",
        "  plt.hist(decisions[1], color='b', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=True, label='Pion (train)')\n",
        "\n",
        "  # make histogram and get error bars\n",
        "  hist, bins = np.histogram(decisions[2], bins=bins, range=low_high, density=True)\n",
        "  width = (bins[1] - bins[0])\n",
        "  center = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "  # plot histogram for signal test sample (Kaon Test)\n",
        "  scale = len(decisions[2]) / sum(hist)\n",
        "  err = np.sqrt(hist * scale) / scale\n",
        "  plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='Kaon (test)')\n",
        "\n",
        "  # make and plot histogram for background test sample (Pion Test)\n",
        "  hist, bins = np.histogram(decisions[3], bins=bins, range=low_high, density=True)\n",
        "  scale = len(decisions[2]) / sum(hist)\n",
        "  err = np.sqrt(hist * scale) / scale\n",
        "  plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='Pion (test)')\n",
        "\n",
        "  plt.xlabel('Classifier output')\n",
        "  plt.ylabel('Arbitrary units')\n",
        "  plt.legend(loc='best')\n",
        "  f.savefig('classifier_'+str(n)+'_output.png')\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "\n",
        "  # Create test and train decisions histograms for kaon\n",
        "  hist_kaon_test, bins = np.histogram( decisions[0], bins=bins, range=low_high, density=False)\n",
        "  hist_kaon_train, bins = np.histogram( decisions[2], bins=bins, range=low_high, density=False)\n",
        "  \n",
        "  # Find chi2 values and error for kaon\n",
        "  hist_kaon_chi2 = np.nan_to_num(np.divide(np.square(np.subtract(hist_kaon_test, hist_kaon_train)),np.add(hist_kaon_test,hist_kaon_train)))\n",
        "  scale = len(decisions[2]) / sum(hist_kaon_chi2)\n",
        "  err_kaon = np.sqrt(hist_kaon_chi2 * scale) / scale\n",
        "  \n",
        "  # Create test and train decisions histograms for pion\n",
        "  hist_pion_test, bins = np.histogram( decisions[1], bins=bins, range=low_high, density=False)\n",
        "  hist_pion_train, bins = np.histogram( decisions[3], bins=bins, range=low_high, density=False)\n",
        "  \n",
        "  # Find chi2 values and error for pion\n",
        "  hist_pion_chi2 = np.nan_to_num(np.divide(np.square(np.subtract(hist_pion_test, hist_pion_train)),np.add(hist_pion_test,hist_pion_train)))\n",
        "  scale = len(decisions[2]) / sum(hist_pion_chi2)\n",
        "  err_pion = np.sqrt(hist_pion_chi2 * scale) / scale\n",
        "  \n",
        "  # Find chi2 for kaon and pion and total\n",
        "  chi2_kaon = sum(hist_kaon_chi2)\n",
        "  chi2_pion = sum(hist_pion_chi2)\n",
        "  chi2_total = chi2_kaon + chi2_pion\n",
        "  err_total = math.sqrt(sum(err_kaon**2 + err_pion**2))\n",
        "\n",
        "  # Record metrics values\n",
        "  metrics[3][0].append(chi2_kaon)\n",
        "  metrics[3][1].append(chi2_pion)\n",
        "  metrics[3][2].append(chi2_total)\n",
        "  metrics[4][0].append(err_kaon)\n",
        "  metrics[4][1].append(err_pion)\n",
        "  metrics[4][2].append(err_total)\n",
        "\n",
        "\n",
        "def batch_plot(name,batch_size,log_master,n,verbosity):\n",
        "    losses = []\n",
        "    accs = []\n",
        "    for k in log_master:\n",
        "      if k[0]=='loss':\n",
        "        losses.append(k[1])\n",
        "        if len(k) > 2 and verbosity == 1:\n",
        "          print(len(k))\n",
        "\n",
        "      elif k[0] == 'accuracy':\n",
        "        accs.append(k[1])\n",
        "        if len(k) > 2 and verbosity == 1:\n",
        "          print(len(k))\n",
        "\n",
        "    chdir('by_batch')\n",
        "\n",
        "    f = plt.figure()\n",
        "    plt.plot(losses)\n",
        "    plt.title(\"Model: \"+name+str(n)+\", Loss vs. batch, batch_size = \"+str(batch_size))\n",
        "    plt.xlabel('batch')\n",
        "    plt.ylabel('loss')\n",
        "    if verbosity==1:\n",
        "      plt.show()\n",
        "    f.savefig('batches_loss_'+str(n)+'.png')\n",
        "\n",
        "    f = plt.figure()\n",
        "    plt.plot(accs)\n",
        "    plt.title(\"Model: \"+name+str(n)+\", Accuracy vs. batch, batch_size = \"+str(batch_size))\n",
        "    plt.xlabel('batch')\n",
        "    plt.ylabel('accuracy')\n",
        "    if verbosity==1:\n",
        "      plt.show()\n",
        "    f.savefig('batches_acc_'+str(n)+'.png')\n",
        "\n",
        "    chdir('..')\n",
        "\n",
        "    # Convert accuracy and loss plots into ROOT histograms\n",
        "    if verbosity == 1:\n",
        "      print(\"Converting batch metrics plots to ROOT...\")\n",
        "\n",
        "    chdir('root_files')\n",
        "\n",
        "    file = uproot.recreate('batches_loss_'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "    file[\"losses\"] = np.histogram2d([i+1 for i in range(len(losses))],losses)\n",
        "\n",
        "    file = uproot.recreate('batches_acc_'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "    file[\"losses\"] = np.histogram2d([i+1 for i in range(len(accs))],accs)\n",
        "\n",
        "    chdir('..')\n",
        "\n",
        "def evaluate_architecture(architecture,name='',num_classes=2,opt=Adam(),batch=320,epochs=10,iterations=10,verbosity=1):\n",
        "\n",
        "  '''\n",
        "  Create a designated folder in which to place all the files for the evaluation of a given architecture.\n",
        "  Record the architecture of the given architecture and plot and record the training metrics (accuracy, \n",
        "  validation accuracy, loss, validation loss) as a function of the training epoch for the given number \n",
        "  of epochs (default 10) with a given batch size (default 320).  Also, evaluate the stability of the \n",
        "  architecture test metrics (accuracy and loss) over a given number of model iterations (default 10).\n",
        "\n",
        "  Matthew McEneaney 3/24/20\n",
        "  '''\n",
        "\n",
        "  # Internal variable definitions\n",
        "  metrics = [ [], [], [],  [ [], [], [] ],  [ [], [], [] ]  ]\n",
        "  minimum = 1\n",
        "  maximum = iterations + 1\n",
        "  step = 1\n",
        "  executable = True\n",
        "  model = architecture\n",
        "\n",
        "  # # Create file directory\n",
        "  for entry in listdir():\n",
        "    if entry == name:\n",
        "      while True:\n",
        "        resp = str(input('The directory ' + name + ' already exists.  Do you wish to overwrite? (y/n):'))\n",
        "        if resp == 'y':\n",
        "          rmtree(name)\n",
        "          executable = True\n",
        "          break\n",
        "        elif resp == 'n':\n",
        "          executable = False\n",
        "          break\n",
        "      break\n",
        "    \n",
        "  if executable == True:\n",
        "    # Make file directory\n",
        "    mkdir(name + '_batchsize' + str(batch))\n",
        "    chdir(name + '_batchsize' + str(batch))\n",
        "    mkdir('root_files')\n",
        "    mkdir('by_epoch')\n",
        "    mkdir('by_batch')\n",
        "    mkdir('Model_Checkpoints')\n",
        "\n",
        "    if name == '':\n",
        "      print('No model name specified!')\n",
        "      name = str(input('Please enter name of desired model: '))\n",
        "\n",
        "    # Record Architecture\n",
        "    model = tf.keras.models.clone_model(model)\n",
        "    write_summary(model,name,verbosity)\n",
        "\n",
        "    # Train architecture several times to assess stability\n",
        "    for n in range(minimum,maximum,step):\n",
        "      if verbosity == 1:\n",
        "        print('#######################\\n# Creating model '+str((n - minimum + 1))+'... #\\n#######################')\n",
        "      model = tf.keras.models.clone_model(architecture)\n",
        " \n",
        "      # Learning Rate Schedule, added 1/28/20 9:20:00\n",
        "      lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.001, decay_steps=1000, decay_rate=1) # , decay_steps=STEPS_PER_EPOCH*1000, decay_rate=1, staircase=False\n",
        "\n",
        "      model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=tf.keras.optimizers.Adam(amsgrad=True), # learning_rate=0.001 decreasing helps eliminate convergence issues but also decreases final quality\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      # Callback added 1/24/20, check out https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau\n",
        "      reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                                               factor=0.1, patience=10, min_lr=0.001,\n",
        "                                                               verbose=1, cooldown=10)\n",
        "      \n",
        "      # Callback added 1/30/20\n",
        "      log_dir=\"logs/fit/\" + 'mymodel' + str(n) # datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "      # Callback added 2/1/20 23:20:00 # no longer working as of 4/1, see callback definition above\n",
        "      log_master = []\n",
        "      m_logger = MetricsLogger(log_master)\n",
        "      \n",
        "      # Callback added 1/26/20\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10,verbose=1,baseline=0.01) #restore_best_weights=True\n",
        "\n",
        "      # Callback added 4/3/20\n",
        "      checkpoint = ModelCheckpoint('Model_Checkpoints', monitor='val_loss', verbose=0, save_best_only=True, mode='min')\n",
        "\n",
        "      history = model.fit(data_train,\n",
        "                          target_train,\n",
        "                          batch_size=batch,\n",
        "                          epochs=epochs,\n",
        "                          verbose=verbosity,\n",
        "                          callbacks=[early_stopping,tensorboard_callback,checkpoint], #,m_logger\n",
        "                          validation_data=(data_val,target_val)) # Error on this line?\n",
        "      \n",
        "      results = model.evaluate(data_test,\n",
        "                               target_test,\n",
        "                               batch_size=batch,\n",
        "                               verbose=verbosity)\n",
        "      \n",
        "      # Plot trainging metrics as a function of epoch\n",
        "      plot_metrics(name,batch,results,history,n,verbosity)\n",
        "\n",
        "      # Evaluate test accuracy, loss, and chi2 for model iteration\n",
        "      evaluate_model(model,metrics,results,n,verbosity)\n",
        "\n",
        "      # Plot loss and accuracy metrics as a function of batch\n",
        "      batch_plot(name,batch,log_master,n,verbosity)\n",
        "\n",
        "      # Save model\n",
        "      model.save('model'+str(n))\n",
        "\n",
        "    # Plot test accuracy, loss, and chi2 as a function of model iteration\n",
        "    plot_stability(metrics,name,batch,verbosity)\n",
        "\n",
        "    # Go back up to home directory\n",
        "    if verbosity == 1:\n",
        "      listdir()\n",
        "    chdir(\"..\")\n",
        "    if verbosity == 1:\n",
        "      getcwd()\n",
        "      print(\"\\nDone\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE0ZMiuUsKO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# chdir('..')\n",
        "print(getcwd())\n",
        "# chdir('..')\n",
        "# chdir('/content/gdrive/My Drive/DIRC_CNN/test/TPU_Test/')\n",
        "# rmtree('tf_cifar_regularized_batchsize320')\n",
        "# mkdir('batchsize32')\n",
        "# chdir('batchsize32')\n",
        "print(listdir())\n",
        "# print(getcwd())\n",
        "# rmtree('tf_cifar_reg_g_batchsize320')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JZ5HSXIodIr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_channels=2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qRv0ZUNNb4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# myModels = [[tf_cifar_regularized(),'tf_cifar_regularized'],\n",
        "#             [tf_cnn_example(),'tf_cnn_example'],\n",
        "#             [tf_cifar_example(),'tf_cifar_example'],\n",
        "#             [tf_cifar_factorization(),'tf_cifar_factorization'],\n",
        "#             [tf_cifar_regwELU(),'tf_cifar_regwELU']]\n",
        "# myModels = [[tf_cnn_basic,'tf_cnn_basic'],\n",
        "#             [tf_cnn_simple_wDO,'tf_simple_wDO'],\n",
        "#             [tf_cnn_simple_wDOandL2,'tf_cnn_simple_wDOandL2']]\n",
        "# myModels = [[tf_cifar_reg_conv3,'tf_cifar_reg_conv3'],\n",
        "#             [tf_cifar_reg_conv4,'tf_cifar_reg_conv4']]\n",
        "# myModels = [[tf_cifar_reg_g,'tf_cifar_reg_g'],\n",
        "#             [tf_cifar_reg_par,'tf_cifar_reg_par']]\n",
        "myModels = [[tf_cifar_reg_g,'tf_cifar_reg_g'],\n",
        "            [tf_cifar_example,'tf_cifar_example']]\n",
        "for entry in myModels:\n",
        "  evaluate_architecture(entry[0],entry[1],epochs=10,batch=320,iterations=10)\n",
        "getcwd()\n",
        "print('Do you wish to continue writing to this directory?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFrlv9ODiDXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsvdzkuniEGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transfer data to appropriately shaped np arrays\n",
        "images_kplus = []\n",
        "images_piplus = []\n",
        "images_flat_kplus = []\n",
        "images_flat_piplus = []\n",
        "\n",
        "times_kplus = []\n",
        "times_kplus_flat = []\n",
        "times_piplus = []\n",
        "times_piplus_flat = []\n",
        "\n",
        "# initialize images\n",
        "image_kplus = np.zeros(shape=(48,144,2))\n",
        "image_piplus = np.zeros(shape=(48,144,2))\n",
        "\n",
        "get_data(data[0][0],data[0][1],50,10000) # currently theta 4 phim 90 split at 50ns limit is 10000 events\n",
        "\n",
        "print(images_kplus[0].shape)\n",
        "print(len(times_kplus))\n",
        "print(np.amax(times_kplus))\n",
        "print(np.amin(times_kplus))\n",
        "print(len(times_piplus))\n",
        "print(np.amax(times_piplus))\n",
        "print(np.amin(times_piplus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdtUIIvJjQpg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assimilate test data\n",
        "print(\"Splitting sample into training and testing subsets\")\n",
        "n_kplus = len(images_kplus)\n",
        "n_piplus = len(images_piplus)\n",
        "n_total = n_kplus + n_piplus\n",
        "\n",
        "data = np.concatenate((images_kplus, images_piplus))\n",
        "data_test = data\n",
        "target = np.concatenate((np.ones(n_kplus), np.zeros(n_piplus)))\n",
        "target_test = target\n",
        "\n",
        "print('data shape = '+str(data.shape))\n",
        "print('target shape = '+str(target.shape))\n",
        "print('unique elements of target = '+str(np.unique(target)))\n",
        "\n",
        "print(\"Testing size = %d\" % len(data_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnlDPNZhjkRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize pixel absolute values to be between 0 and 1\n",
        "data_test = data_test / data.max()\n",
        "\n",
        "print(\"Data shape: \"+str(data_test.shape))\n",
        "print(\"Target shape: \"+str(target_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1N9nq76V2Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('/content/gdrive/My Drive/DIRC_CNN/test/Data_Split_Test/tf_cifar_regularized_batchsize320/')\n",
        "results = model.evaluate(data_test,\n",
        "                               target_test,\n",
        "                               batch_size=batch,\n",
        "                               verbose=verbosity)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}