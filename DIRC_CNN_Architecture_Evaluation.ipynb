{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIRC_CNN_Architecture_Evaluation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mfmceneaney/DIRC_COLAB/blob/master/DIRC_CNN_Architecture_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-uxFFigUIQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using GPU speeds up CNN training\n",
        "# Go to Edit > Notebook Settings > Hardware Accelerator and select GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HpK-suowGUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\" > Installing uproot...\")\n",
        "!pip install uproot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvCGwbOcT6oQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import uproot\n",
        "\n",
        "from os import chdir, getcwd, mkdir, listdir, rmdir\n",
        "from shutil import rmtree\n",
        "\n",
        "# Standard scientific Python imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.applications import DenseNet121, ResNet50V2, VGG16, VGG19, MobileNet, MobileNetV2, NASNetLarge, NASNetMobile\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0sofRSRTWfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99DmdI8VVHXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize images\n",
        "image_kplus = np.zeros(shape=(48,144))\n",
        "image_piplus = np.zeros(shape=(48,144))\n",
        "images_kplus = []\n",
        "images_piplus = []\n",
        "images_flat_kplus = []\n",
        "images_flat_piplus = []\n",
        "\n",
        "#h2_kplus = ROOT.TH2F(\"kplus\",\"Hit Pattern K+; PixelRow; PixelCol\",144,-0.5,143.5,48,-0.5,47.5)\n",
        "#h2_piplus = ROOT.TH2F(\"piplus\",\"Hit Pattern pi+; PixelRow; PixelCol\",144,-0.5,143.5,48,-0.5,47.5)\n",
        "\n",
        "# get data from files\n",
        "print(\"Opening files\")\n",
        "piplus = uproot.open(\"/content/gdrive/My Drive/piplus_p3_theta4_flat.root\")[\"dircml_flat\"]\n",
        "kplus = uproot.open(\"/content/gdrive/My Drive/kplus_p3_theta4_flat.root\")[\"dircml_flat\"]\n",
        "\n",
        "# fill histogram with # photon hits\n",
        "#plt.axis([0, 250, 0, 5000])\n",
        "#plt.hist(NPixelsArr, 4000) #, 0, 100) #bins='auto')\n",
        "#plt.title(\"Histogram with 'auto' bins\")\n",
        "#plt.show()\n",
        "\n",
        "eventMax = 10000\n",
        "eventCounter = 0\n",
        "\n",
        "print(\"Filling image arrays\")\n",
        "# loop over kplus events\n",
        "for (PixelTimes,PixelRows,PixelCols) in zip(kplus.array(\"PixelTime\"),kplus.array(\"PixelRow\"),kplus.array(\"PixelCol\")):\n",
        "    #print(\"event\")\n",
        "    image_single = np.zeros(shape=(48,144))\n",
        "    \n",
        "    # loop over pixels within event\n",
        "    for (PixelTime,PixelRow,PixelCol) in zip(PixelTimes,PixelRows,PixelCols):\n",
        "        #print(\"PixelTime,Row,Col = %f,%d,%d\" % (PixelTime,PixelRow,PixelCol))\n",
        "        image_kplus[PixelCol,PixelRow] += 1\n",
        "        image_single[PixelCol,PixelRow] = PixelTime\n",
        "        #h2_kplus.Fill(PixelRow,PixelCol)\n",
        "    \n",
        "    # after each event\n",
        "    images_kplus.append(image_single)\n",
        "    images_flat_kplus.append(np.reshape(image_single, 6912))\n",
        "    \n",
        "    eventCounter = eventCounter+1\n",
        "    if eventCounter > eventMax:\n",
        "        break\n",
        "\n",
        "eventCounter = 0\n",
        "        \n",
        "# loop over piplus events\n",
        "for (PixelTimes,PixelRows,PixelCols) in zip(piplus.array(\"PixelTime\"),piplus.array(\"PixelRow\"),piplus.array(\"PixelCol\")):\n",
        "    #print(\"event\")\n",
        "    image_single = np.zeros(shape=(48,144))\n",
        "    \n",
        "    # loop over pixels within event\n",
        "    for (PixelTime,PixelRow,PixelCol) in zip(PixelTimes,PixelRows,PixelCols):\n",
        "        #print(\"PixelTime,Row,Col = %f,%d,%d\" % (PixelTime,PixelRow,PixelCol))\n",
        "        image_piplus[PixelCol,PixelRow] += 1\n",
        "        image_single[PixelCol,PixelRow] = PixelTime\n",
        "        #h2_piplus.Fill(PixelRow,PixelCol)\n",
        "    \n",
        "    # after each event\n",
        "    images_piplus.append(image_single)\n",
        "    images_flat_piplus.append(np.reshape(image_single, 6912))\n",
        "    \n",
        "    eventCounter = eventCounter+1\n",
        "    if eventCounter > eventMax:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKxwtrYWaxub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw cumulative image\n",
        "plt.axis([-0.5, 143.5, -0.5, 47.5])\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title(\"Hit Pattern Histogram K+ (top) and Pi+ (bottom)\")\n",
        "plt.imshow(image_kplus, cmap='viridis')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title(\"\")\n",
        "plt.imshow(image_piplus, cmap='viridis')\n",
        "plt.show()\n",
        "\n",
        "# draw single event image\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.title(\"Hit Time Histogram K+ (top) and Pi+ (bottom)\")\n",
        "plt.imshow(images_kplus[0], cmap='viridis')\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.title(\"\")\n",
        "plt.imshow(images_piplus[0], cmap='viridis')\n",
        "#plt.show()\n",
        "\n",
        "# draw ROOT image\n",
        "#can = ROOT.TCanvas(\"cc\",\"cc\",600,500)\n",
        "#can.Divide(1,2)\n",
        "#can.cd(1)\n",
        "#h2_kplus.Draw(\"colz\")\n",
        "#can.cd(2)\n",
        "#h2_piplus.Draw(\"colz\")\n",
        "#can.Print(\"HitPattern.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ4-MSLja1iY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split samples into training and testing\n",
        "print(\"Splitting sample into training and testing subsets\")\n",
        "split = np.array((80,10,10)) # Percentages into which to split data (% train, % val, % test)\n",
        "n_kplus = len(images_kplus)\n",
        "n_piplus = len(images_piplus)\n",
        "n_total = n_kplus + n_piplus\n",
        "print(n_kplus / split)\n",
        "\n",
        "print(n_kplus // split)\n",
        "print(n_piplus // split)\n",
        "images_train_kplus = images_kplus[:int(n_kplus * split[0] / 100)]\n",
        "images_train_piplus = images_piplus[:int(n_piplus * split[0] / 100)]\n",
        "n_train_kplus = len(images_train_kplus)\n",
        "n_train_piplus = len(images_train_piplus)\n",
        "n_train_total = n_train_kplus + n_train_piplus\n",
        "\n",
        "images_val_kplus = images_kplus[int(n_kplus * split[0] / 100):int(n_piplus * (split[0] + split[1]) / 100)]\n",
        "images_val_piplus = images_piplus[int(n_piplus * split[0] / 100):int(n_piplus * (split[0] + split[1]) / 100)]\n",
        "n_val_kplus = len(images_val_kplus)\n",
        "n_val_piplus = len(images_val_piplus)\n",
        "n_val_total = n_val_kplus + n_val_piplus\n",
        "\n",
        "images_test_kplus = images_kplus[int(n_piplus * (split[0] + split[1]) / 100):]\n",
        "images_test_piplus = images_piplus[int(n_piplus * (split[0] + split[1]) / 100):]\n",
        "n_test_kplus = len(images_test_kplus)\n",
        "n_test_piplus = len(images_test_piplus)\n",
        "n_test_total = n_test_kplus + n_test_piplus\n",
        "\n",
        "data_train = np.concatenate((images_train_kplus, images_train_piplus))\n",
        "data_val = np.concatenate((images_val_kplus, images_val_piplus))\n",
        "data_test = np.concatenate((images_test_kplus, images_test_piplus))\n",
        "data = np.concatenate((images_kplus, images_piplus))\n",
        "\n",
        "target_train = np.concatenate((np.ones(n_train_kplus), np.zeros(n_train_piplus)))\n",
        "target_val = np.concatenate((np.ones(n_val_kplus), np.zeros(n_val_piplus)))\n",
        "target_test = np.concatenate((np.ones(n_test_kplus), np.zeros(n_test_piplus)))\n",
        "target = np.concatenate((np.ones(n_kplus), np.zeros(n_piplus)))\n",
        "\n",
        "\n",
        "print(data_train.shape)\n",
        "print(data.shape)\n",
        "print(target.shape)\n",
        "print(np.unique(target))\n",
        "\n",
        "print(\"Training size = %d\" % len(data_train))\n",
        "print(\"Testing size = %d\" % len(data_test))\n",
        "\"\"\"\n",
        "print(images_kplus[1].shape)\n",
        "print(images_piplus[0].shape)\n",
        "\n",
        "print(data.max())\n",
        "print(data_train.max())\n",
        "print(data_test.max())\n",
        "print(data.min())\n",
        "print(data_train.min())\n",
        "print(data_test.min())\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BETYfnkURpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape Data to have a single channel\n",
        "data_train_images = data_train.reshape(len(data_train), 48, 144, 1)\n",
        "target_train_images = target_train.reshape(len(target_train))\n",
        "\n",
        "data_val_images = data_val.reshape(len(data_val), 48, 144, 1)\n",
        "target_val_images = target_val.reshape(len(target_val))\n",
        "\n",
        "data_test_images = data_test.reshape(len(data_test), 48, 144, 1)\n",
        "target_test_images = target_test.reshape(len(target_test))\n",
        "\n",
        "# Normalize pixel absolute values to be between 0 and 1\n",
        "data_train_images = data_train_images / data.max()\n",
        "data_val_images = data_val_images / data.max()\n",
        "data_test_images = data_test_images / data.max()\n",
        "\n",
        "#print(data_train_images.max())\n",
        "#print(data_test_images.max())\n",
        "\n",
        "print(\"Data shape: \"+str(data_train_images.shape))\n",
        "print(\"Target shape: \"+str(target_train_images.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW3YbgZIkokD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################\n",
        "# Evaluation function definitions #\n",
        "# Matthew McEneaney               #\n",
        "# 1/12/20                         #\n",
        "###################################\n",
        "\n",
        "def write_summary(model,name,verbosity):\n",
        "  '''\n",
        "  Write model architecture summary into a .txt file.\n",
        "  '''\n",
        "\n",
        "  def myFunc(text):\n",
        "    outF.write(text+'\\n')\n",
        "  \n",
        "  # Write summary to file\n",
        "  outF = open(name+\"_summary.txt\", \"w\")\n",
        "  model.summary(print_fn=myFunc)\n",
        "  outF.close()\n",
        "  if verbosity == 1:\n",
        "    model.summary()\n",
        "\n",
        "def plot_metrics(name,batch_size,results,history,n,verbosity):\n",
        "  '''\n",
        "  Plot training metrics (accuracy, validation accuracy, \n",
        "  loss, validation, loss) as a function of training \n",
        "  epoch and save in .png and .root file formats.\n",
        "  '''\n",
        "\n",
        "  # Plot as .png\n",
        "  f = plt.figure(figsize=(16,10))\n",
        "  val = plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'],'--', color=val[0].get_color())\n",
        "  val = plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'],'--', color=val[0].get_color())\n",
        "  plt.title('Model: '+name+' '+str(n)+' metrics, batch_size = '+str(batch_size))\n",
        "  plt.ylabel('Arbitrary Units')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['accuracy', 'val_accuracy','loss', 'val_loss'], loc='best')\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"epochs_metrics\"+str(n)+\".png\")\n",
        "\n",
        "  # Convert accuracy and loss plots into ROOT histograms\n",
        "  if verbosity == 1:\n",
        "    print(\"Converting training epochs plots to ROOT...\")\n",
        "\n",
        "  epochs = np.array([i for i in range(len(history.history['accuracy']))])\n",
        "\n",
        "  file = uproot.recreate('epochs_accuracy_'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"acc\"] = np.histogram2d(history.history['accuracy'],epochs)\n",
        "\n",
        "  file = uproot.recreate('epochs_val_accuracy'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"val_acc\"] = np.histogram2d(history.history['val_accuracy'],epochs)\n",
        "\n",
        "  file = uproot.recreate('epochs_loss'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"loss\"] = np.histogram2d(history.history['loss'],epochs)\n",
        "\n",
        "  file = uproot.recreate('epochs_val_loss'+str(n)+'.root', compression=uproot.ZLIB(4))\n",
        "  file[\"val_loss\"] = np.histogram2d(history.history['val_loss'],epochs)\n",
        "\n",
        "def plot_stability(metrics,name,batch_size,verbosity):\n",
        "  '''\n",
        "  Plot the test metrics (accuracy, loss, chi2) recorded for \n",
        "  several model iterations in both .png and .root file formats.\n",
        "  '''\n",
        "\n",
        "  if verbosity == 1:\n",
        "    print(\"Plotting purity and \\u03C7\\u00b2\")\n",
        "\n",
        "  # Plot accuracy\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[1],'b.', scaley = False)\n",
        "  plt.title(\"Model: \"+name+\", Accuracy, batch_size = \"+str(batch_size))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_acc.png\")\n",
        "\n",
        "  # Plot loss\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[2],'b.', scaley = False)\n",
        "  plt.title(\"Model: \"+name+\", Loss, batch_size = \"+str(batch_size))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_loss.png\")\n",
        "\n",
        "  # Plot kaon chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][0],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Kaon \\u03C7\\u00b2, batch_size = \"+str(batch_size))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_kaon.png\")\n",
        "\n",
        "  # Plot pion chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][1],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Pion \\u03C7\\u00b2, batch_size = \"+str(batch_size))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_pion.png\")\n",
        "\n",
        "  # Plot total chi2\n",
        "  f = plt.figure()\n",
        "  plt.plot(metrics[0],metrics[3][2],'b.')\n",
        "  plt.title(\"Model: \"+name+\", Total \\u03C7\\u00b2, Layers = 32, batch_size = \"+str(batch_size))\n",
        "  plt.xlabel(\"Iteration\")\n",
        "  plt.ylabel(\"Arbitrary units\")\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "  f.savefig(\"stability_chi2_tot.png\")\n",
        "    \n",
        "  # Convert accuracy and loss plots into ROOT histograms\n",
        "  if verbosity == 1:\n",
        "    print(\"Converting stability plots to ROOT...\")\n",
        "\n",
        "  file = uproot.recreate(\"stability_acc.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"acc\"] = np.histogram2d(metrics[0],metrics[1])\n",
        "\n",
        "  file = uproot.recreate(\"stability_loss.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"loss\"] = np.histogram2d(metrics[0],metrics[2])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_kaon.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2k\"] = np.histogram2d(metrics[0],metrics[3][0])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_pion.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2p\"] = np.histogram2d(metrics[0],metrics[3][1])\n",
        "\n",
        "  file = uproot.recreate(\"stability_chi2_tot.root\", compression=uproot.ZLIB(4))\n",
        "  file[\"chi2t\"] = np.histogram2d(metrics[0],metrics[3][2])\n",
        "\n",
        "def evaluate_model(model,metrics,n,verbosity):\n",
        "\n",
        "  '''\n",
        "  Record the decisions of the CNN (kaon or pion) and \n",
        "  test metrics (accuracy and loss) for a single training\n",
        "  iteration. Also calculate and record the chi2 and error\n",
        "  for kaon, pion decisions and all decisions respectively.\n",
        "  '''\n",
        "\n",
        "  # Keep track of results for stability plots\n",
        "  metrics[0].append(n)\n",
        "  metrics[1].append(results[1])\n",
        "  metrics[2].append(results[0])\n",
        "\n",
        "  # Evaluate performance by comparing test and training classifier (model) response\n",
        "  decisions = []\n",
        "  for (X,y) in ((data_train_images, target_train_images), (data_test_images, target_test_images)):\n",
        "      d1 = model.predict_proba(X[y>0.5])[:,1].ravel()\n",
        "      d2 = model.predict_proba(X[y<0.5])[:,1].ravel()\n",
        "      decisions += [d1, d2]\n",
        "      \n",
        "  print(\"Filling histograms...\")\n",
        "\n",
        "  bins = 100\n",
        "  low = min(np.min(d) for d in decisions)\n",
        "  high = max(np.max(d) for d in decisions)\n",
        "  low_high = (low,high)\n",
        "  f = plt.figure()\n",
        "  plt.clf()\n",
        "  # Plot training decisions\n",
        "  plt.hist(decisions[0], color='r', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=True, label='Kaon (train)')\n",
        "  plt.hist(decisions[1], color='b', alpha=0.5, range=low_high, bins=bins, histtype='stepfilled', density=True, label='Pion (train)')\n",
        "\n",
        "  # make histogram and get error bars\n",
        "  hist, bins = np.histogram(decisions[2], bins=bins, range=low_high, density=True)\n",
        "  width = (bins[1] - bins[0])\n",
        "  center = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "  # plot histogram for signal test sample (Kaon Test)\n",
        "  scale = len(decisions[2]) / sum(hist)\n",
        "  err = np.sqrt(hist * scale) / scale\n",
        "  plt.errorbar(center, hist, yerr=err, fmt='o', c='r', label='Kaon (test)')\n",
        "\n",
        "  # make and plot histogram for background test sample (Pion Test)\n",
        "  hist, bins = np.histogram(decisions[3], bins=bins, range=low_high, density=True)\n",
        "  scale = len(decisions[2]) / sum(hist)\n",
        "  err = np.sqrt(hist * scale) / scale\n",
        "  plt.errorbar(center, hist, yerr=err, fmt='o', c='b', label='Pion (test)')\n",
        "\n",
        "  plt.xlabel('Classifier output')\n",
        "  plt.ylabel('Arbitrary units')\n",
        "  plt.legend(loc='best')\n",
        "  f.savefig('classifier_'+str(n)+'_output.png')\n",
        "  if verbosity == 1:\n",
        "    plt.show()\n",
        "\n",
        "  # Create test and train decisions histograms for kaon\n",
        "  hist_kaon_test, bins = np.histogram( decisions[0], bins=bins, range=low_high, density=False)\n",
        "  hist_kaon_train, bins = np.histogram( decisions[2], bins=bins, range=low_high, density=False)\n",
        "  \n",
        "  # Find chi2 values and error for kaon\n",
        "  hist_kaon_chi2 = np.nan_to_num(np.divide(np.square(np.subtract(hist_kaon_test, hist_kaon_train)),np.add(hist_kaon_test,hist_kaon_train)))\n",
        "  scale = len(decisions[2]) / sum(hist_kaon_chi2)\n",
        "  err_kaon = np.sqrt(hist_kaon_chi2 * scale) / scale\n",
        "  \n",
        "  # Create test and train decisions histograms for pion\n",
        "  hist_pion_test, bins = np.histogram( decisions[1], bins=bins, range=low_high, density=False)\n",
        "  hist_pion_train, bins = np.histogram( decisions[3], bins=bins, range=low_high, density=False)\n",
        "  \n",
        "  # Find chi2 values and error for pion\n",
        "  hist_pion_chi2 = np.nan_to_num(np.divide(np.square(np.subtract(hist_pion_test, hist_pion_train)),np.add(hist_pion_test,hist_pion_train)))\n",
        "  scale = len(decisions[2]) / sum(hist_pion_chi2)\n",
        "  err_pion = np.sqrt(hist_pion_chi2 * scale) / scale\n",
        "  \n",
        "  # Find chi2 for kaon and pion and total\n",
        "  chi2_kaon = sum(hist_kaon_chi2)\n",
        "  chi2_pion = sum(hist_pion_chi2)\n",
        "  chi2_total = chi2_kaon + chi2_pion\n",
        "  err_total = math.sqrt(sum(err_kaon**2 + err_pion**2))\n",
        "\n",
        "  # Record metrics values\n",
        "  metrics[3][0].append(chi2_kaon)\n",
        "  metrics[3][1].append(chi2_pion)\n",
        "  metrics[3][2].append(chi2_total)\n",
        "  metrics[4][0].append(err_kaon)\n",
        "  metrics[4][1].append(err_pion)\n",
        "  metrics[4][2].append(err_total)\n",
        "\n",
        "def evaluate_architecture(model,name,opt=Adam(),batch_size=320,epochs=100,iterations=10,verbosity=1):\n",
        "\n",
        "  '''\n",
        "  Create a designated folder in which to place all the files for the evaluation of a given architecture.\n",
        "  Record the architecture of the given architecture and plot and record the training metrics (accuracy, \n",
        "  validation accuracy, loss, validation loss) as a function of the training epoch for the given number \n",
        "  of epochs (default 100) with a given batch size (default 320).  Also, evaluate the stability of the \n",
        "  architecture test metrics (accuracy and loss) over a given number of model iterations (default 10).\n",
        "  '''\n",
        "\n",
        "  # Internal variable definitions\n",
        "  metrics = [ [], [], [],  [ [], [], [] ],  [ [], [], [] ]  ]\n",
        "  minimum = 1\n",
        "  maximum = iterations + 1\n",
        "  step = 1\n",
        "  executable = False\n",
        "\n",
        "  # Create file directory\n",
        "  chdir('/content/gdrive/My Drive/')\n",
        "  for entry in listdir():\n",
        "    if entry == name:\n",
        "      while True:\n",
        "        resp = str(input('This file already exists.  Do you wish to continue? (y/n):'))\n",
        "        if resp == 'y':\n",
        "          rmtree(name)\n",
        "          executable = True\n",
        "          break\n",
        "        elif resp == 'n':\n",
        "          executable = False\n",
        "          break\n",
        "      break\n",
        "    \n",
        "  if executable == True:\n",
        "    # Make file directory\n",
        "    mkdir(name)\n",
        "    chdir(name)\n",
        "\n",
        "    # Record model architecture\n",
        "    write_summary(model,name,verbosity)\n",
        "\n",
        "    # Train architecture several times to assess stability    \n",
        "    for n in range(minimum,maximum,step):\n",
        "      if verbosity == 1:\n",
        "        print('###################### \\\n",
        "        # Creating model '+str((n - minimum + 1))+'... #\\n \\\n",
        "        ######################')\n",
        "      \n",
        "      model.compile(loss='sparse_categorical_crossentropy',\n",
        "                    optimizer=opt,\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "      history = model.fit(data_train_images,\n",
        "                          target_train_images,\n",
        "                          epochs=numEpochs,\n",
        "                          verbose=verbosity,\n",
        "                          batch_size=batch_size, \n",
        "                          validation_data=(data_val_images,target_val_images))\n",
        "      \n",
        "      results = model.evaluate(data_test_images,\n",
        "                               target_test_images,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=verbosity)\n",
        "      \n",
        "      # Plot trainging metrics as a function of epoch\n",
        "      plot_metrics(name,batch_size,results,history,n,verbosity)\n",
        "\n",
        "      # Evaluate test accuracy, loss, and chi2 for model iteration\n",
        "      evaluate_model(model,metrics,results,n,verbosity)\n",
        "\n",
        "    # Plot test accuracy, loss, and chi2 as a function of model iteration\n",
        "    plot_stability(metrics,name,batch_size,verbosity)\n",
        "\n",
        "    # Go back up to home directory\n",
        "    if verbosity == 1:\n",
        "      listdir()\n",
        "    chdir(\"..\")\n",
        "    if verbosity == 1:\n",
        "      getcwd()\n",
        "      print(\"\\nDone\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOpyeDVRJUs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#####################\n",
        "# Model Definitions #\n",
        "# Matthew McEneaney #\n",
        "# 1/12/20           #\n",
        "#####################\n",
        "\n",
        "############################################### TensorFlow Example Architecture ###########################################\n",
        "# check out https://www.tensorflow.org/tutorials/images/cnn\n",
        "def tf_cnn_example(num_classes):\n",
        "  # CNN takes 3 dimensional tensor input (image height, image width, color channel)\n",
        "  model = models.Sequential()\n",
        "  # Padding='same' significantly improves results\n",
        "  # model.add(layers.Masking(mask_value=0.1, input_shape=(48, 144, 1)))\n",
        "  model.add(layers.Conv2D(32, (3, 3), strides=(1,1), padding='same', activation='relu', input_shape=(48, 144, 1)))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D((2, 2)))\n",
        "  model.add(layers.Conv2D(64, (3, 3), strides=(1,1), activation='relu'))\n",
        "  model.add(layers.Flatten())\n",
        "  # model.add(layers.Dropout(0.1))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  # model.add(layers.Dropout(0.1))\n",
        "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "################################### Simple CNN for CIFAR-10 Dataset, use Adam Optimizer #######################################\n",
        "# Check out https://keras.io/examples/cifar10_resnet/ for examples of this architecture and others\n",
        "def tf_cifar_example(num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=(48, 144, 1)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model\n",
        "\n",
        "  # # Example used this optimizer\n",
        "  # initiate RMSprop optimizer\n",
        "  # opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n",
        "\n",
        "####################################### Try Bottleneck Layers ############################################\n",
        "def tf_cifar_bottleneck(num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=(48, 144, 1)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (1, 1))) # Bottleneck layers\n",
        "  model.add(Conv2D(64, (3, 3))) # ...\n",
        "  model.add(Conv2D(64, (1, 1))) # ...\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        "  return model\n",
        "\n",
        "#################################### Try Factorization Layers #########################################\n",
        "def tf_cifar_factorization(num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                  input_shape=(48, 144, 1)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (1, 3))) # Factorization layers\n",
        "  model.add(Conv2D(64, (3, 1))) # ...\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (1, 3))) # Factorization layers\n",
        "  model.add(Conv2D(64, (3, 1))) # ...\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  return model\n",
        "#####################################################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qRv0ZUNNb4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myModels = [[tf_cnn_example(2),'tf_cnn_example'],[tf_cifar_example(2),'tf_cifar_example'],[tf_cifar_factorization(2),'tf_cifar_factorization']]\n",
        "# myModels = [[tf_cifar_factorization(2),'tf_cifar_factorization']]\n",
        "for entry in myModels:\n",
        "  evaluate_architecture(entry[0],entry[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1N9nq76V2Ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}